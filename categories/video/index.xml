<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>video on Michael Gleicher's Web Page</title><link>/categories/video/</link><description>Recent content in video on Michael Gleicher's Web Page</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 22 May 2023 17:58:17 -0500</lastBuildDate><atom:link href="/categories/video/index.xml" rel="self" type="application/rss+xml"/><item><title>RangedIK: An Optimization-based Robot Motion Generation Method for Ranged-Goal Tasks</title><link>/video/2023_rangedik/</link><pubDate>Mon, 22 May 2023 17:58:17 -0500</pubDate><guid>/video/2023_rangedik/</guid><description/></item><item><title>Trinary Tools for Continuously Valued Classifiers (CBoxer Talk Video)</title><link>/video/2022_cboxer_talk/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>/video/2022_cboxer_talk/</guid><description>This video accompanies our paper Trinary tools for continuously valued binary classifiers about the CBoxer system. This video is the &amp;ldquo;talk&amp;rdquo; from the online VisXAI workshop that was part of Pacific Vis 2022.
For more information see the Boxer Project Page.</description></item><item><title>Trinary Tools for Continuously Valued Classifiers (Paper Accompaniment Video)</title><link>/video/2022_cboxer_paper/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>/video/2022_cboxer_paper/</guid><description>This video accompanies our paper Trinary tools for continuously valued binary classifiers about the CBoxer system.
For more information see the Boxer Project Page.</description></item><item><title>Informing Real-Time Corrections in Corrective Shared Autonomy Through Expert Demonstrations</title><link>/video/2021_corrections2/</link><pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate><guid>/video/2021_corrections2/</guid><description>Video accompaniment to the RAL/IROS 2021 Paper Informing Real-Time Corrections in Corrective Shared Autonomy Through Expert Demonstrations.</description></item><item><title>Corrective Shared Autonomy for Addressing Task Variability</title><link>/video/2021_corrections1/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>/video/2021_corrections1/</guid><description>Video accompaniment to the RAL/ICRA 2021 Paper Corrective Shared Autonomy for Addressing Task Variability.</description></item><item><title>Recognizing Orientation Slip in Human Demonstrations</title><link>/video/2021_slip/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>/video/2021_slip/</guid><description>Video accompaniment to the ICRA 2021 Paper Recognizing Orientation Slip in Human Demonstrations.</description></item><item><title>Boxer: Interactive Comparison of Classifiers (Talk Video)</title><link>/video/2020_boxer_talk/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>/video/2020_boxer_talk/</guid><description>This video was the &amp;ldquo;talk&amp;rdquo; for EuroVis 2020 that presented our paper Boxer: Interactive Comparison of Classifier Results. Because EuroVis was held virtually (due to COVID-19), the talk is this 12 minute video.
For more information see the Boxer Project Page.</description></item><item><title>Boxer: Interactive Comparison of Classifiers (Paper Video)</title><link>/video/2020_boxer-demo/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>/video/2020_boxer-demo/</guid><description>See also the talk video.</description></item><item><title>Remote Telemanipulation with Adapting Viewpoints in Visually Complex Environments</title><link>/video/2019_rss-camera/</link><pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate><guid>/video/2019_rss-camera/</guid><description/></item><item><title>Shared-Control-Based Bimanual Robot Manipulation</title><link>/video/2019_bimanual/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>/video/2019_bimanual/</guid><description>This video demonstrates our system for interactive control of bimanual robots, published in Science Robotics, June 2019.</description></item><item><title>Stampede: A Discrete-Optimization Method for Solving Pathwise-Inverse Kinematics</title><link>/video/2019_stampede/</link><pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate><guid>/video/2019_stampede/</guid><description/></item><item><title>User-Guided Offline Synthesis of Robot Arm Motion from 6-DoF Paths</title><link>/video/2019_userguide/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>/video/2019_userguide/</guid><description/></item><item><title>Inferring Geometric Constraints in Human Demonstrations</title><link>/video/2018_inferring/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>/video/2018_inferring/</guid><description/></item><item><title>Robust Replay of Human Demonstrations Using Identified Constraints</title><link>/video/2018_robust_replay/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>/video/2018_robust_replay/</guid><description>This video went along with a paper that was never published (it was submitted to ICRA &amp;lsquo;19). The methods are described in Guru&amp;rsquo;s Thesis. The constraint inference approach is discussed in this paper on Axiv.</description></item><item><title>RelaxedIK: Real-time Synthesis of Accurate and Feasible Robot Arm Motion</title><link>/video/2018_relaxedik/</link><pubDate>Sat, 30 Jun 2018 00:00:00 +0000</pubDate><guid>/video/2018_relaxedik/</guid><description/></item><item><title>An Autonomous Dynamic Camera Method for Effective Remote Teleoperation</title><link>/video/2018-hri-camera/</link><pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate><guid>/video/2018-hri-camera/</guid><description>This is the video accompaniment to our best-paper award winning paper An Autonomous Dynamic Camera Method for Effective Remote Teleoperation from HRI 2018. It demonstrates a system that allows the user to control a robot manipulator and see what they are doing by having a second robot control a video camera that watches the action.
See also the later video from the system that added more intelligence to the camera control, published at RSS 2019.</description></item><item><title>A Motion Retargeting Method for Effective Mimicry-based Teleoperation of Robot Arms</title><link>/video/2017_mimicry/</link><pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate><guid>/video/2017_mimicry/</guid><description/></item><item><title>Authoring Directed Gaze for Full-body Motion Capture</title><link>/video/2016_gazeediting/</link><pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate><guid>/video/2016_gazeediting/</guid><description/></item><item><title>Serendip Rank Viewer</title><link>/video/2015_serendiprankview/</link><pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate><guid>/video/2015_serendiprankview/</guid><description>A demonstration of Serendip&amp;rsquo;s RankViewer. Serendip is a corpus exploration tool that uses probabilistic topic modeling as a lens through which to view documents. The demonstration uses a topic model generated on a corpus of 1,080 digitized English texts published between 1530 and 1799.</description></item><item><title>Splatterplots: Overcoming Overdraw in Scatter Plots</title><link>/video/2013_splatterplots/</link><pubDate>Sun, 01 Sep 2013 00:00:00 +0000</pubDate><guid>/video/2013_splatterplots/</guid><description/></item><item><title>Stylized and Performative Gaze for Character Animation</title><link>/video/2013_stylizedgaze/</link><pubDate>Wed, 15 May 2013 00:00:00 +0000</pubDate><guid>/video/2013_stylizedgaze/</guid><description/></item><item><title>Online Real-Time Presentation of Virtual Experiences for External Viewers</title><link>/video/2012_realtime_presentvr/</link><pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate><guid>/video/2012_realtime_presentvr/</guid><description/></item><item><title>Effective Replays and Summarization of Virtual Experiences</title><link>/video/2012_replayvr/</link><pubDate>Sun, 01 Apr 2012 00:00:00 +0000</pubDate><guid>/video/2012_replayvr/</guid><description/></item><item><title>SubSpace Video Stabilization (ToG 2011)</title><link>/video/2011_subspacestabilization/</link><pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate><guid>/video/2011_subspacestabilization/</guid><description>Video from our 2011 Transactions on Graphics Paper.
This system provided a practical approach to warp stabilization. It is the key technical piece that lead to Adobe&amp;rsquo;s Warp Stabilizer.
This work was done with Feng Liu, Michael Gleicher, Jue Wang, Hailin Jin, and Aseem Agarwala.</description></item><item><title>Content-Preserving Warps for 3D Video Stabilization</title><link>/video/2009_warp_stable/</link><pubDate>Wed, 01 Jul 2009 00:00:00 +0000</pubDate><guid>/video/2009_warp_stable/</guid><description>This is a video demonstration from our SIGGRAPH 2009 paper &amp;ldquo;Content-Preserving Warps for 3D Video Stabilization&amp;rdquo;. This is an early predecessor to the technology that became Adobe&amp;rsquo;s Warp Stabilizer (although, that is really based on a later paper).</description></item><item><title>Re-Cinematography (2007)</title><link>/video/2008_recinematography/</link><pubDate>Mon, 01 Oct 2007 00:00:00 +0000</pubDate><guid>/video/2008_recinematography/</guid><description>This video demonstrates Re-Cinematography - our approach for post-process video stabilization that tries to produce high-quality camera motions. The original paper (at MultiMedia 2007) won the &amp;ldquo;best in track award,&amp;rdquo; and was invited to be extended as a journal paper in TOMCCAP.
Note: this is taken from a &amp;ldquo;FLV&amp;rdquo; file so, at one point it was highly compressed using 2007 technology. It was before we had reliable H.264 (or better).</description></item><item><title>Parametric Motion Graphs</title><link>/video/2007_pmg/</link><pubDate>Mon, 30 Apr 2007 00:00:00 +0000</pubDate><guid>/video/2007_pmg/</guid><description>We present an example-based motion synthesis technique that generates continuous streams of high-fidelity, controllable motion for interactive applications, such as video games.</description></item><item><title>Virtual Videography</title><link>/video/2007_vv-mm/</link><pubDate>Wed, 28 Feb 2007 00:00:00 +0000</pubDate><guid>/video/2007_vv-mm/</guid><description>We present automated system called Virtual Videography that employs the art of videography to mimic videographer-produced lecture videos, while being unobtrusive when recording.
The system is described in a 2007 Paper.</description></item><item><title>Splicing Upper-Body Actions with Locomotion</title><link>/video/2006_splicing/</link><pubDate>Fri, 15 Sep 2006 00:00:00 +0000</pubDate><guid>/video/2006_splicing/</guid><description>We present a simple and efficient technique for synthesizing high-fidelity motions by attaching, or splicing,the upper-body action of one motion example to the lower-body locomotion of another.</description></item><item><title>Automated Extraction and Parameterization of Motions in Large Data Sets</title><link>/video/2004_parammotions/</link><pubDate>Sun, 01 Aug 2004 00:00:00 +0000</pubDate><guid>/video/2004_parammotions/</guid><description>We provide automated methods for identifying logically similar motions in a data set and using them to build a continuous and intuitively parameterized space of motions.</description></item><item><title>Virtual Videography 2004 (early result)</title><link>/video/2004_vv04/</link><pubDate>Wed, 28 Jan 2004 00:00:00 +0000</pubDate><guid>/video/2004_vv04/</guid><description>Note: see the Virtual Videography video page for a more recent result.</description></item><item><title>Magic Boards (1)</title><link>/video/2003_magicboard1/</link><pubDate>Wed, 10 Dec 2003 00:00:00 +0000</pubDate><guid>/video/2003_magicboard1/</guid><description/></item><item><title>Magic Boards (2)</title><link>/video/2003_magicboard2/</link><pubDate>Wed, 10 Dec 2003 00:00:00 +0000</pubDate><guid>/video/2003_magicboard2/</guid><description/></item><item><title>Magic Boards (3)</title><link>/video/2003_magicboard3/</link><pubDate>Wed, 10 Dec 2003 00:00:00 +0000</pubDate><guid>/video/2003_magicboard3/</guid><description/></item><item><title>Stylizing Motion with Drawings</title><link>/video/2003_cartoon/</link><pubDate>Fri, 01 Aug 2003 00:00:00 +0000</pubDate><guid>/video/2003_cartoon/</guid><description/></item><item><title>Building Efficient, Accurate Character Skins from Examples</title><link>/video/2003_efficientskins/</link><pubDate>Tue, 15 Jul 2003 00:00:00 +0000</pubDate><guid>/video/2003_efficientskins/</guid><description/></item><item><title>Deformation Sensitive Decimation</title><link>/video/2003_decimation/</link><pubDate>Tue, 01 Jul 2003 00:00:00 +0000</pubDate><guid>/video/2003_decimation/</guid><description/></item><item><title>Flexible Automatic Motion Blending with Registration Curves</title><link>/video/2003_regcurves/</link><pubDate>Tue, 01 Jul 2003 00:00:00 +0000</pubDate><guid>/video/2003_regcurves/</guid><description/></item><item><title>Motion Editing with Paths and Tiles</title><link>/video/2003_tiles/</link><pubDate>Thu, 01 May 2003 00:00:00 +0000</pubDate><guid>/video/2003_tiles/</guid><description>This video shows off work we did in 2003, but was never published. The student graduated before we were able to complete the paper.</description></item><item><title>Direct Manipulation of Interactive Character Skins</title><link>/video/2003_directskin/</link><pubDate>Tue, 01 Apr 2003 00:00:00 +0000</pubDate><guid>/video/2003_directskin/</guid><description/></item><item><title>Snap Together Motion: Assembling Run-Time Animation</title><link>/video/2003_stm/</link><pubDate>Tue, 01 Apr 2003 00:00:00 +0000</pubDate><guid>/video/2003_stm/</guid><description>We present an approach to character motion called Snap-Together Motion that addresses the unique demands of virtual environments. Snap-Together Motion (STM) preprocesses a corpus of motion capture examples into a set of short clips that can be concatenated to make continuous streams of motion.</description></item><item><title>Virtual Videography (2003 early version)</title><link>/video/2003_vv/</link><pubDate>Wed, 29 Jan 2003 00:00:00 +0000</pubDate><guid>/video/2003_vv/</guid><description>The main idea behind Virtual Videography is to automatically edit video of some event that would be nice to record, but too costly or intrusive to place a camera crew, such as a classroom lecture. (Note: Newer video at Virtual Videography).</description></item><item><title>Footskate Cleanup for Motion Capture Editing</title><link>/video/2002_footskate/</link><pubDate>Tue, 06 Aug 2002 00:00:00 +0000</pubDate><guid>/video/2002_footskate/</guid><description>Video from our 2002 paper &amp;ldquo;Footskate cleanup for motion capture editing&amp;rdquo;. This work was published at the Symposium on Computer Animation.
Many motion capture editing operations result in the feet of the character moving when they ought to remain planted. We present a simple, efficient algorithm for removing this footskate.</description></item><item><title>Motion Graphs</title><link>/video/2002_mographs/</link><pubDate>Tue, 06 Aug 2002 00:00:00 +0000</pubDate><guid>/video/2002_mographs/</guid><description/></item><item><title>HijackGL: Reconstructing from Streams for Stylized Rendering</title><link>/video/2002_hijackgl/</link><pubDate>Mon, 01 Jul 2002 00:00:00 +0000</pubDate><guid>/video/2002_hijackgl/</guid><description/></item><item><title>Simplicial Families of Drawings</title><link>/video/2001_simplicial/</link><pubDate>Thu, 01 Nov 2001 00:00:00 +0000</pubDate><guid>/video/2001_simplicial/</guid><description/></item><item><title>Motion Path Editing</title><link>/video/2001_pathediting/</link><pubDate>Thu, 01 Mar 2001 00:00:00 +0000</pubDate><guid>/video/2001_pathediting/</guid><description>Warning: this is a poor quality AVI upload.</description></item><item><title>Retargetting Motion to New Characters</title><link>/video/1998_st-retarget/</link><pubDate>Sat, 01 Aug 1998 00:00:00 +0000</pubDate><guid>/video/1998_st-retarget/</guid><description/></item><item><title>Spacetime Swing</title><link>/video/1998_st-swing/</link><pubDate>Sat, 01 Aug 1998 00:00:00 +0000</pubDate><guid>/video/1998_st-swing/</guid><description/></item><item><title>Projective Registration with Difference Decomposition</title><link>/video/1997_tracking/</link><pubDate>Tue, 01 Jul 1997 00:00:00 +0000</pubDate><guid>/video/1997_tracking/</guid><description/></item><item><title>Motion Editing With Spacetime Constraints</title><link>/video/1997_st-moedit/</link><pubDate>Sat, 01 Mar 1997 00:00:00 +0000</pubDate><guid>/video/1997_st-moedit/</guid><description>Yes. This really was all in real-time on a circa 1996 Macintosh. I have no idea how I made it go so fast.
This video is from the 1997 Interactive 3D Symposium Paper. An earlier version of the paper actually appeared as a 1998 journal paper. The journal paper mainly has 2D examples, and focuses on the non-interactive cases.</description></item><item><title>Image Snapping</title><link>/video/1995_snapping/</link><pubDate>Tue, 01 Aug 1995 00:00:00 +0000</pubDate><guid>/video/1995_snapping/</guid><description/></item><item><title>A Differential Approach to Graphical Manipulation</title><link>/video/1994_thesis/</link><pubDate>Sat, 01 Jan 1994 00:00:00 +0000</pubDate><guid>/video/1994_thesis/</guid><description/></item><item><title>Through-the-Lens Camera Control</title><link>/video/1992_ttl/</link><pubDate>Wed, 01 Jul 1992 00:00:00 +0000</pubDate><guid>/video/1992_ttl/</guid><description/></item><item><title>Briar</title><link>/video/1991_briar/</link><pubDate>Tue, 01 Jan 1991 00:00:00 +0000</pubDate><guid>/video/1991_briar/</guid><description/></item></channel></rss>