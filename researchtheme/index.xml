<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Research Themes on Michael Gleicher's Web Page</title><link>/researchtheme/</link><description>Recent content in Research Themes on Michael Gleicher's Web Page</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/researchtheme/index.xml" rel="self" type="application/rss+xml"/><item><title>Video, Animation and Image Authoring</title><link>/researchtheme/authoring/</link><pubDate>Sun, 28 Jun 2020 18:49:48 -0500</pubDate><guid>/researchtheme/authoring/</guid><description>Our goal is to make it easier for people to create useable images and video. For example, we have developed methods for improving pictures and video as a post-process (e.g. removing shadows and stabilizing video). We have also worked on adapting imagery for use in new settings (e.g. image and video retargeting or automatic video editing) and making use of large image collections (e.g. intestingness detection or panorama finding).</description></item><item><title>Visualizing English Print</title><link>/researchtheme/vep/</link><pubDate>Sun, 28 Jun 2020 18:49:37 -0500</pubDate><guid>/researchtheme/vep/</guid><description>To drive our data science efforts, we took a specific application: working with English literature scholars to develop approaches to working with large collections of historical texts.</description></item><item><title>Perceptual Principles for Visualization</title><link>/researchtheme/perceptprinc/</link><pubDate>Sun, 28 Jun 2020 18:49:29 -0500</pubDate><guid>/researchtheme/perceptprinc/</guid><description>Understanding how people see can inform how we should design visualizations. We have been exploring how recent results in perception (e.g., ensemble encoding) can be exploited to create novel visualization designs, and how principles of perception can inform visualization designs.</description></item><item><title>Communicative Characters</title><link>/researchtheme/commchar/</link><pubDate>Sun, 28 Jun 2020 18:49:13 -0500</pubDate><guid>/researchtheme/commchar/</guid><description>We are working on better ways to synthesize human motions to make animated characters (both on screen and robots) that are better able to communicate. Generally, we focus on trying to make use of collections of examples (such as motion capture) to build models that allow us to generate novel movements, or to define models of communicative motions.</description></item><item><title>Usable VR and AR</title><link>/researchtheme/usablearvr/</link><pubDate>Sun, 28 Jun 2020 18:48:50 -0500</pubDate><guid>/researchtheme/usablearvr/</guid><description>Virtual Reality (VR) and Augmented Reality (AR) are interesting display devices that are becoming practical. We are exploring how to design VR and AR applications that can address application tasks, as well as to develop new mechanisms that will make these displays more useful across a broad range of applications.</description></item><item><title>Visualizing Comparisons for Data Science</title><link>/researchtheme/visualcomp/</link><pubDate>Sun, 28 Jun 2020 18:48:41 -0500</pubDate><guid>/researchtheme/visualcomp/</guid><description>Data interpretation tasks often involve making comparisons among the data, or can be thought of as comparisons. We are developing better visualization tools for performing comparisons for various data challenges, as well as to developing better methods for inventing new designs.</description></item><item><title>Interacting with Machine Learning</title><link>/researchtheme/interactlearn/</link><pubDate>Sun, 28 Jun 2020 18:45:00 -0500</pubDate><guid>/researchtheme/interactlearn/</guid><description>People interact with machine learning systems in many ways: they must build them, debug them, diagnose them, decide to trust them, gain insights on their data from them, etc. We are exploring this in both directions: How do we build machine learning tools into interactive data analysis in order to help people interpret large and complex data? How do we build interaction tools that can help people construct and diagnose machine learning models?</description></item><item><title>Communicating Physical Interactions</title><link>/researchtheme/commphysinteractions/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>/researchtheme/commphysinteractions/</guid><description>We are working on ways for people and robots to communicate to each other about how objects should be manipulated in the world. Manipulations necessarily involve physical interactions (e.g., forces must be applied correctly). We are exploring ways for people to tell robots how to act with appropriate forces (e.g., to teach manipulation skills) as well as for robots to communicate back to people about the actions they are performing.</description></item></channel></rss>